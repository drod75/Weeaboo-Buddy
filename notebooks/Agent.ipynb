{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/drod75/Weeaboo-Buddy/blob/main/notebooks/Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVyBVxxoaZEo"
   },
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CQucXmWBWehl",
    "outputId": "7647d604-5282-4e3e-9c94-1b610c6935a0"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-community tavily-python python-dotenv langchain-google-genai langgraph langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ9Zj9cqXZa_"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jem-_jkEack0"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FMhnYbGX9VO"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "118Hdl96X_ZX"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if t[\"name\"] not in self.tools:  # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knwjumocYd9y"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from google.colab import userdata\n",
    "\n",
    "import os\n",
    "\n",
    "tkey = userdata.get(\"TAVILY_API_KEY\")\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = tkey\n",
    "\n",
    "gkey = userdata.get(\"GOOGLE_API_KEY\")\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = gkey\n",
    "\n",
    "tool = TavilySearchResults(max_results=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRGHL8-uYDhH",
    "outputId": "26298ffd-f81c-475b-8ef7-9f3294e4720f"
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Who won the super bowl in 2024? In what state is the winning team headquarters located? \\\n",
    "What is the GDP of that state? Answer each question.\"\n",
    ")\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "abot = Agent(\n",
    "    model,\n",
    "    [tool],\n",
    "    system=\"You are an American football expert who does research of every team and there stats each season.\",\n",
    ")\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcxhQhD5afi4"
   },
   "source": [
    "## Testing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GvjE63qaQyT",
    "outputId": "6fb72f90-00b5-4e5d-9d6d-c35ed7bcca59"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a crunchyroll data analyst who specialzes in researching various different types of anime to help questions for fans,\n",
    "as well as other analysts who specialize in drawing up graph and statistics.\n",
    "\"\"\"\n",
    "query = \"Who is Zoro from One Piece\"\n",
    "\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dIVls1ha9Nt",
    "outputId": "7c9e4d05-10ce-469b-e90c-5b2c121afbb7"
   },
   "outputs": [],
   "source": [
    "! pip install jikanpy-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C0IMukpmbNht",
    "outputId": "ccc14172-a5b9-4224-bdc5-ee7e07145eab"
   },
   "outputs": [],
   "source": [
    "from jikanpy import Jikan\n",
    "\n",
    "jikan = Jikan()\n",
    "\n",
    "bebop = jikan.anime(1)\n",
    "bebop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "957elOXxbZD8",
    "outputId": "7f59435e-36c2-4861-c62d-3e111757cbc1"
   },
   "outputs": [],
   "source": [
    "current_season = jikan.seasons(extension=\"now\")\n",
    "current_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QUQNZMT9bt0v",
    "outputId": "cb7c817a-bf43-4324-dd27-43b662a3b498"
   },
   "outputs": [],
   "source": [
    "search_result = jikan.search(\"anime\", \"Boku no Piko\")\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc1mBl_Eb7Xh"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_anime(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Name:\n",
    "      search_anime\n",
    "    Description:\n",
    "      Search for an anime by a name, the result will be a json of possile solutions, from best to worst match.\n",
    "    Args:\n",
    "      query: anime name\n",
    "    \"\"\"\n",
    "\n",
    "    search_result = jikan.search(\"anime\", query)\n",
    "    return search_result\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_stats_by_id(id: int) -> dict:\n",
    "    \"\"\"\n",
    "    Name:\n",
    "      get_stats_by_id\n",
    "    Description:\n",
    "      Get the stats of an anime by it's id\n",
    "    Args:\n",
    "      id: anime id\n",
    "    \"\"\"\n",
    "\n",
    "    result = jikan.anime(id)\n",
    "    return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_seasonal() -> dict:\n",
    "    \"\"\"\n",
    "    Name:\n",
    "      get_seasonal\n",
    "    Description:\n",
    "      Get the anime from this season\n",
    "    Args:\n",
    "      None\n",
    "    \"\"\"\n",
    "    current_season = jikan.seasons(extension=\"now\")\n",
    "    return current_season\n",
    "\n",
    "\n",
    "tavily = TavilySearchResults(max_results=4)\n",
    "tools = [search_anime, get_stats_by_id, get_seasonal, tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO5_BNH5dPlb",
    "outputId": "be1d9278-402f-44f2-c032-6f809416fc3a"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a crunchyroll data analyst who specialzes in researching various different types of anime to help questions for fans,\n",
    "as well as other analysts who specialize in drawing up graph and statistics.\n",
    "\"\"\"\n",
    "query = \"What are the anime from this current season?\"\n",
    "\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "abot = Agent(model, tools, system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQVbPjFtdZA8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJmfokcFrsc4P3husag/G+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
